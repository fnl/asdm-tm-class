{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Deep learning for text mining\n",
    "\n",
    "Practical course material for the ASDM Class 09 (Text Mining) by Florian Leitner.\n",
    "\n",
    "Â© 2017 Florian Leitner. All rights reserved.\n",
    "\n",
    "During this course, we've worked with two datasets: the *20 Newsgroups* (day 1) and the *Reuters-21578* (day 3) datasets.\n",
    "In this last exercise, we will re-run the classification with a neural network and try to beat [the best *published* results](http://clair.si.umich.edu/%7Eradev/papers/tc.pdf) from models *other* than neural networks.\n",
    "(If you like, particularly 20 Newsgroups is the equivalent of \"MINST\" for computer vision, but for text mining.)\n",
    "\n",
    "For those two corpora, the best (non-neural) baselines achieve around 90% accuracy on the 20 Newsgroups  coprus, using the official (roughly 3:2) split (see below!). Even we, on day one, achieved around 85% accuracy on this set.\n",
    "\n",
    "(For Retuers-21578, the state-of-the-art (\"ante deep learning\") was 94% micro-averaged $F_1$ Score using the official ModApte split, but only selecting documents among the ten most frequent categories (200 or more documents), and a micro-averaged 89% $F_1$ Score is using all 90 categories.\n",
    "And, obviously, it is important that the evaluation follows a single multilabel classification setting, not 10 or 90 individual binary classification problems... Most deep lerning literature only focuses on the simpler 10-categories subset of Reuter-21578, because otherwise there are too few examples to work with, so the number you should keep in mind for that set when reading a new deep learning paper using it is the 94% micro-averaged $F_1$ score.)\n",
    "\n",
    "Our goal for this last tutorial will be trying to beat the state-of-the-art with deep learning, to better understand how powerful these very new machine learning techniques are.\n",
    "\n",
    "## Installation and setup\n",
    "\n",
    "For this last \"preview\" course, we will be using [Keras](https://keras.io/) \"on Theano\" (or CNTK or TensorFlow, as you prefer [1]) to understand how to build a simple classifier with a neural network - that short-handedly beats all results you've seen so far.\n",
    "\n",
    "[1] Theano is probably well suited for teaching/learning networks, while Microsoft's CNTK is probably best suited for langauge modeling (of those three choices!) and Google's TensorFlow is certainly the best all-rounder and probably the most popular. If you already have something else than Theano (the default, AFAIK) set up with Keras, go with that.\n",
    "\n",
    "[Installing Keras](https://keras.io/#installation) is simple:\n",
    "\n",
    "```bash\n",
    "conda install keras\n",
    "# or:\n",
    "pip3 install keras\n",
    "```\n",
    "\n",
    "Similarly, [installing Theano](http://deeplearning.net/software/theano/install.html#install) is easy, too, although getting it to work with your Nvidia GPU - assuming you have one in your laptop in the first place - might be more fidegty (see Theano's instructions about `libgpuarray` if you are not using `conda`, which does that for you via `pygpu`) - but still a lot simpler than with most other neural network libraries (if they are not supported by `conda`...)\n",
    "\n",
    "```bash\n",
    "conda install theano # no Nvidia GPU\n",
    "conda install theano pygpu # with Nvidia GPU support\n",
    "# or:\n",
    "pip install Theano[doc]\n",
    "```\n",
    "\n",
    "Note that your instructor has a laptop without an Nvidia GPU, and this tutorial has been \"calibarated\" so that it can be run without one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# only to ensure its installed:\n",
    "import theano # or your favorite choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final tutorial is taken from very useful [blog](https://blog.keras.io/index.html) about \"doing\" Deep Learning with Keras (in Python) and you might find different details in the [original tutorial post](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) (but do not that it makes a rather serious mistake during preprocessing - see \"Data preparation\" below).\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "On day 3 we already have seen various ways to build your own word embeddings.\n",
    "Here we will take a \"shortcut\" and use a precomputed set of (single) word embeddings, [GloVe](https://nlp.stanford.edu/projects/glove/), which is [hosted and distributed by Stanford](https://nlp.stanford.edu/data/).\n",
    "We'll \"pretend\" that we are just trying to get a quick prototype running to see if our idea (multi-label classification of documents with convolutional networks) works.\n",
    "Once we've ensured it does, we can \"scale\" it to larger and/or more specific embeddings [1] and deeper networks (we'll use just a 1D conv net here).\n",
    "\n",
    "[1] Generally, if you have the time and resources to build your own embeddings, you will always be better off with that avenue.\n",
    "As discussed before, is particularly true for capturing named entities and idioms that are highly specific for your specific domain and where the meaning of the collocations go beyond that of the single words.\n",
    "\n",
    "If you are more of a fan of the FastText model, language-specific 300 dimensional word embeddings for a great many languages were [made available by Facebook's research department](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md) recently.\n",
    "\n",
    "For now, we'll just stick to the smallest possible set (in the hopes that your local laptops can cope with the data): please download `glove.6B.zip`, word embeddings created from WikiPedia, and will just use the 50 dimensional embeddings (again, in the hopes that this works on your laptops; if you have a GPU, use 100 dimensional set for a [little] performance gain - while the 300d set contributes no gains over 100d). Note the download is almost 1GB (\"As homework\" you can experiment with larger GloVe collections [42B, 840B] to see if that helps improve the final performance.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50 # use 100 on a GPU, or to get max. performance\n",
    "WORD_VECTOR_FILE = 'glove.6B.%dd.txt' % EMBEDDING_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Found 400000 word vectors with dim=100.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline --no-import-all\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(WORD_VECTOR_FILE) as stream:\n",
    "    for line in stream:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "    stream.close()\n",
    "\n",
    "print('Found %s word vectors with dim=%s.' % (\n",
    "    len(embeddings_index),\n",
    "    next(iter(embeddings_index.values())).shape[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have loaded 400,000 word vectors.\n",
    "\n",
    "## Corpus setup\n",
    "\n",
    "Here, we shall differ from the blog post in a positive sense:\n",
    "We will keep using the headers, which contain the subject, and can often give us critcal hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ TRAIN ------------\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "LABEL: rec.autos = 7\n",
      "\n",
      "\n",
      "------------ TEST ------------\n",
      "From: adamsj@gtewd.mtv.gtegsc.com\n",
      "Subject: Re: Homosexuality issues in Christianity\n",
      "Reply-To: adamsj@gtewd.mtv.gtegsc.com\n",
      "Organization: GTE Govt. Systems, Electronics Def. Div.\n",
      "Lines: 18\n",
      "\n",
      "In article <May.13.02.29.39.1993.1505@geneva.rutgers.edu>, revdak@netcom.com (D. Andrew Kille) writes:\n",
      "> Of course the whole issue is one of discernment.  It may be that Satan\n",
      "> is trying to convince us that we know more than God.  Or it may be that\n",
      "> God is trying (as God did with Peter) to teach us something we don't\n",
      "> know- that \"God shows no partiality, but in every nation anyone who fears\n",
      "> him and does what is right is acceptable to him.\" (Acts 10:34-35).\n",
      "> \n",
      "> revdak@netcom.com\n",
      "\n",
      "Fine, but one of the points of this entire discussion is that \"we\"\n",
      "(conservative, reformed christians - this could start an argument...\n",
      "But isn't this idea that homosexuality is ok fairly \"new\" [this\n",
      "century] ? Is there any support for this being a viable viewpoint\n",
      "before this century? I don't know.) don't believe that homosexuality\n",
      "is \"acceptable to Him\". So your scripture quotation doesn't work for\n",
      "\"us\".\n",
      "\n",
      "-jeff adams-\n",
      "\n",
      "LABEL: soc.religion.christian = 15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "train = fetch_20newsgroups()\n",
    "test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "print(\"------------ TRAIN ------------\")\n",
    "print(train.data[0].strip())\n",
    "print(\"\\nLABEL:\", train.target_names[train.target[0]],\n",
    "      \"=\", train.target[0])\n",
    "print(\"\\n\\n------------ TEST ------------\")\n",
    "print(test.data[-1].strip())\n",
    "print(\"\\nLABEL:\", test.target_names[test.target[-1]],\n",
    "      \"=\", test.target[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the GloVe word vectors don't come with \"apostrophe forms\" and instead expand those contractions to full words; Here, we will do the same (thereby differing from the blog post)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "lexicon = (\n",
    "    (re.compile(r\"\\bdon't\\b\"), \"do not\"),\n",
    "    (re.compile(r\"\\bit's\\b\"), \"it is\"),\n",
    "    (re.compile(r\"\\bi'm\\b\"), \"i am\"),\n",
    "    (re.compile(r\"\\bi've\\b\"), \"i have\"),\n",
    "    (re.compile(r\"\\bcan't\\b\"), \"cannot\"),\n",
    "    (re.compile(r\"\\bdoesn't\\b\"), \"does not\"),\n",
    "    (re.compile(r\"\\bthat's\\b\"), \"that is\"),\n",
    "    (re.compile(r\"\\bdidn't\\b\"), \"did not\"),\n",
    "    (re.compile(r\"\\bi'd\\b\"), \"i would\"),\n",
    "    (re.compile(r\"\\byou're\\b\"), \"you are\"),\n",
    "    (re.compile(r\"\\bisn't\\b\"), \"is not\"),\n",
    "    (re.compile(r\"\\bi'll\\b\"), \"i will\"),\n",
    "    (re.compile(r\"\\bthere's\\b\"), \"there is\"),\n",
    "    (re.compile(r\"\\bwon't\\b\"), \"will not\"),\n",
    "    (re.compile(r\"\\bwoudn't\\b\"), \"would not\"),\n",
    "    (re.compile(r\"\\bhe's\\b\"), \"he is\"),\n",
    "    (re.compile(r\"\\bthey're\\b\"), \"they are\"),\n",
    "    (re.compile(r\"\\bwe're\\b\"), \"we are\"),\n",
    "    (re.compile(r\"\\blet's\\b\"), \"let us\"),\n",
    "    (re.compile(r\"\\bhaven't\\b\"), \"have not\"),\n",
    "    (re.compile(r\"\\bwhat's\\b\"), \"what is\"),\n",
    "    (re.compile(r\"\\baren't\\b\"), \"are not\"),\n",
    "    (re.compile(r\"\\bwasn't\\b\"), \"was not\"),\n",
    "    (re.compile(r\"\\bwouldn't\\b\"), \"would not\"),\n",
    ")\n",
    "\n",
    "def fix_apostrophes(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    for pattern, replacement in lexicon:\n",
    "        text = pattern.sub(replacement, text)\n",
    "\n",
    "    return text\n",
    "\n",
    "text_train = list(map(fix_apostrophes, train.data))\n",
    "text_test = list(map(fix_apostrophes, test.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "Keras comes with its own [test preprocessing facilities](https://keras.io/preprocessing/text/) (very much like `gensim`'s, which we've seen already)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit extraction to the words found in the **training set** (only [1]!), selecting the `NUM_UNIQ_WORDS` most frequent tokens as feature *candidates* (see padding below) only; It turns out we can live with less than the blog post uses and still get nearly the \"same\" results, and that we should remove the single quote apostrophe character (`'`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_UNIQ_WORDS = 10000\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=NUM_UNIQ_WORDS,\n",
    "    lower=False, # use True if you don't fix_apostrophes\n",
    "    # Keras' default filters don't remove the single quote\n",
    "    # apostrophe (') - filter it, as GloVe doesn't know it \n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] About that above remark regarding \"only\", and the next step:\n",
    "The original tutorial makes a rather typical mistake - it fits the word extraction on the test data, too.\n",
    "Therefore, the blog post's preprocessing takes its own test data into account.\n",
    "\n",
    "But the test data is only there to *apply* and *evaluate* your model, not to evaluate it.\n",
    "\"In real life\", you don't actually get a chance to tune your setup against the test data.\n",
    "Therefore, such errors lead to overley optimistic evaluation results of classifiers and machine learning models (and possibly \"irreproducible results\" for your fellow researchers).\n",
    "As a sad word of warning, a far too big proportion of peer-reviewed research contains such trivial, but mission-critical errors, and for \"papers\" on arXiv and similar sites, the only right assumption is that the evaluation results presented are probably wrong, unless you can prove yourself otherwise.\n",
    "In this particular case, the mistake in the post is isn't too dramatic, but it does have a noticable impact (some percentage points performance loss).\n",
    "\n",
    "In any case, you should **never \"fit\" or \"tune\"** anything in your (preprocessing or not) pipeline **using test data**, as you are guaranteed to get an overly optimistic result (that will not hold against truly \"unseen\" data, because you now have *overfitted* your model).\n",
    "At least if you are building a real-life, \"production\" classifier, this single advice will be probably the most imporant thing you need to keep in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126595 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# only fit on training data!\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "# but texts_to_sequences will only be using\n",
    "# the NUM_UNIQ_WORDS most frequent ones!\n",
    "\n",
    "# generate bag-of-word vectors from both train and test\n",
    "seq_train = tokenizer.texts_to_sequences(text_train)\n",
    "seq_test = tokenizer.texts_to_sequences(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our sequences are now integers, where each integer is an index (from `tokenizer.word_index`), in the order in which the tokens appeared in the document, and only for the \"selected\" (`NUM_UNIQ_WORDS`) tokens.\n",
    "\n",
    "Next, we chop our sequences to equally sized vectors of `MAX_SEQ_LEN`, thereby generating the actual input \"document vector\" for our model.\n",
    "Unlike the blog post, though, we will take the *first* `MAX_SEQ_LEN` words (by setting `truncating='post'`), not the last.\n",
    "That is, we will be using at most the first `MAX_SEQ_LEN` words of each document, and each element in the vector will be an index for that word at the given position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 11314\n",
      "Shape of training data tensor: (11314, 1000)\n",
      "Shape of taraining label tensor: (11314, 20)\n",
      "\n",
      "Size of test set: 7532\n",
      "Shape of test data tensor: (7532, 1000)\n",
      "Shape of test label tensor: (7532, 20)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "MAX_SEQ_LEN = 1000\n",
    "\n",
    "data_train = pad_sequences(seq_train, maxlen=MAX_SEQ_LEN, truncating='post')\n",
    "data_test = pad_sequences(seq_test, maxlen=MAX_SEQ_LEN, truncating='post')\n",
    "\n",
    "labels_train = to_categorical(np.asarray(train.target))\n",
    "labels_test = to_categorical(np.asarray(test.target))\n",
    "print('Size of training set:', len(train.data))\n",
    "print('Shape of training data tensor:', data_train.shape)\n",
    "print('Shape of taraining label tensor:', labels_train.shape)\n",
    "print('\\nSize of test set:', len(test.data))\n",
    "print('Shape of test data tensor:', data_test.shape)\n",
    "print('Shape of test label tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 11,311 training examples/documents for training, characterized as a 1000-dimensional word *count* vecotor, and a 20-dimensional label vector (20 Newsgroups...) for each example/document.\n",
    "And we have 7532 test examples/documents for evaluating our approach.\n",
    "\n",
    "**Note that here, too, we significantly deviate from the blog post.** Instead of using the official ~3:~2 split (for each 3 training articles, you leave aside 2 test articles, roughly) on the 20 Newsgroups corpus, the post uses a much \"easier\" 4:1 random split.\n",
    "Between the preprocessing issues and the non-standard split, these two issues alone explain why the blog post achieves such hair-raisingly good results with such a simple architecture, with way higher performance scores than anything previously seen in the research literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0] ... [ 113  186  203 1438 1327    2   14   37   58 7828]\n"
     ]
    }
   ],
   "source": [
    "print(data_train[0][:10], \"...\", data_train[0][-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you see above are word indexes - and/or leading zeros, if the document didn't contain enough words.\n",
    "\n",
    "Finally, we rename the dataset to follow the same nomencalture as used in the blog post (instead of using the random 1:4 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = data_train\n",
    "y_train = labels_train\n",
    "x_val = data_test\n",
    "y_val = labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "This is probably the \"technically\" most interesting part - you will see just how incredibly easy it is to transform our word count vectors into proper word embedding vectors and plug that into a neural network with Keras.\n",
    "\n",
    "First, we generate the weight matrix for the connections between the input (the padded \"document vector\" sequences) and the embedding layer. Those weights therefore will be the  GloVe word embedding vectors, one for each of the `NUM_UNIQ_WORDS` possible words we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_uniq_input_words = min(NUM_UNIQ_WORDS, len(tokenizer.word_index))\n",
    "embedding_matrix = np.zeros((num_uniq_input_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i == NUM_UNIQ_WORDS:\n",
    "        break\n",
    "        \n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    \n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    #else:\n",
    "    #    # words not found in the index use all-zero vectors\n",
    "    #    print(\"not in index:\", word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need a bunch of \"components\" used to build our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [embedding layer](https://keras.io/layers/embeddings/); Note that the tutorial set `trainable` to `False`, to avoid that the embeddings get change; However, there is no conceivable reason to do that, and in fact, performance suffers if held constant. So this layer will expand our one-dimensional `MAX_SEQ_LEN` \"document word index vectors\" into `MAX_SEQ_LEN` times `EMBEDDING_DIM` matrices, replacing the index values with the appropriate GloVe word embedding vector, hence it is a a simple \"vector lookup\" this layer is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(\n",
    "    num_uniq_input_words,\n",
    "    EMBEDDING_DIM,\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQ_LEN,\n",
    "    trainable=False) # no need to keep embeddings fixed (as in the blog post)!\n",
    "# use True if you have a GPU, False if not or to get max. performance\n",
    "# note that with False, your final accuracy will be 5-10% lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging our input tensor shape (\"layer\") and the embeddings lookup layer together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_input = Input(shape=(MAX_SEQ_LEN,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that's it - at this point, you've seen how easy it is to \"transform\" a collection of words in a document into a semantically meaningful tensor ready for deep learning.\n",
    "\n",
    "Next, we shall set up a conv net with three 5 word-window-sized, 1-dimensional (documents are \"linear\") convolutions, each followed by a max-pooling regularization (with a factor of 2, 5, and 35 max-pooling of our words).\n",
    "Why? Because you are an expert, or read tons of literature to find the \"best\" architecture, or (here) are simply following a blog post...\n",
    "In essence, these max-pooled convolutions \"compress\" the `MAX_SEQ_LEN` document vector into one one single number (times the `EMBEDDING_DIM`).\n",
    "\n",
    "The WildML blog has excellent posts explaining the [basics of text classiciation with conv nets](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/) and an example [conv net for text classification](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/). Note that the posts assumes you have a 2-dimenstional word-sentence document matrix as input, while we are using a 1-dimensional document vector (and hence use `Conv1D`, not `Conv2D`).\n",
    "\n",
    "With that in mind, we make a few minor tweaks to the blog post:\n",
    "\n",
    "- Instead of using fixed, 128 dimensional outputs, we stick with our embedding dimension.\n",
    "- We remove the final ReLU layer and add a Dropout layer to get stronger regularization (which will allows us to keep training for more epochs, getting to a higher accuracy - but also training longer...).\n",
    "\n",
    "And after the convolutions, we \"roll out\" (aka. flatten) the convolutions into a single-dimensional \"document vector\".\n",
    "\n",
    "(Tip: for better -but far more complex- architectures look at the Conclusions...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1d_13 input: (None, 1000, 100) - output: (None, 996, 100)\n",
      "max_pooling1d_13 input: (None, 996, 100) - output: (None, 199, 100)\n",
      "conv1d_14 input: (None, 199, 100) - output: (None, 195, 100)\n",
      "max_pooling1d_14 input: (None, 195, 100) - output: (None, 39, 100)\n",
      "conv1d_15 input: (None, 39, 100) - output: (None, 35, 100)\n",
      "dropout_5 input: (None, 35, 100) - output: (None, 35, 100)\n",
      "max_pooling1d_15 input: (None, 35, 100) - output: (None, 1, 100)\n",
      "flatten_5 input: (None, 1, 100) - output: (None, 100)\n"
     ]
    }
   ],
   "source": [
    "x = embedded_sequences\n",
    "\n",
    "for layer in [\n",
    "    Conv1D(EMBEDDING_DIM, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Conv1D(EMBEDDING_DIM, 5, activation='relu'),\n",
    "    MaxPooling1D(5),\n",
    "    Conv1D(EMBEDDING_DIM, 5, activation='relu'),\n",
    "    Dropout(0.5), # very critical to tune this hyper-parameter! (.25 - .5)\n",
    "    MaxPooling1D(35),\n",
    "    Flatten(),\n",
    "]:\n",
    "    x = layer(x)\n",
    "    print(layer.name,\n",
    "          \"input:\", layer.input_shape,\n",
    "          \"- output:\", layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that to get to the `1 x EMBEDDING_DIM` final size, the `MAX_SEQ_LEN` needs to be `1000` (so you *might* need/want to fiddle with the parameters of the max-pooled convolutions if you change `MAX_SEQ_LEN`). For `MAX_SEQ_LEN = 1000`, the final output tensor from the convolusions is `1 x EMBEDDING_DIM` due to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for conv layers, subtract kernel size minus one to get the output size\n",
    "# for max pool layers, divide by the pool size\n",
    "(((1000-5)//5-4)//5-4)//35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the final output - a vector with `EMBEDDING_DIM` numbers - we apply a softmax transformation down to the number of category lables, thereby giving us the propabilities for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cats = len(train.target_names)\n",
    "preds = Dense(n_cats, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we place the layers into a proper [Keras model](https://keras.io/models/model/), using multi-class cross-entropy training handled by an [RMS-prop gradient descent optimizer](http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop). And we ask Keras to report the current (training and validation) accuracy at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 1000, 100)         1000000   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 996, 100)          50100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 199, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 195, 100)          50100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 39, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 35, 100)           50100     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 35, 100)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 1, 100)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                2020      \n",
      "=================================================================\n",
      "Total params: 1,152,320\n",
      "Trainable params: 1,152,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now, we train... Note that we here actually commit another crime: We evaluate the model on the test data! Instead, if done properly, you should be evaluating on a subset of the *training* data, and only once your entire model is build, evaluate it on the official test data.\n",
    "But that would leave our model with even less training data to work with...\n",
    "\n",
    "At the end of the day, we can probably assume that no researcher gets stuff published without that \"cheat\" [1], we will just do the same: Evaluate training progress directly against (aka. \"by overfitting the model on\") the test data.\n",
    "\n",
    "[1] And that explains why community evaluations exist: To tell you the \"real truth\"! Because only then nobody gets access to the test data before the final evaluation. That is, community evaluations are much like a the more serious, grown-up versions of the now popular Kaggle tasks.\n",
    "\n",
    "**WARNING**: This step can take *a **very long** time* unless you have a (or more...) GPU[s] (just see how long you wait for the next Epoch and multiply by n. epochs to estimate the overall runtime). Using 100 (or 300) dimensional embeddings and training them makes the epochs take much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11314 samples, validate on 7532 samples\n",
      "Epoch 1/20\n",
      "11314/11314 [==============================] - 175s - loss: 2.6601 - acc: 0.1409 - val_loss: 2.3280 - val_acc: 0.2411\n",
      "Epoch 2/20\n",
      "11314/11314 [==============================] - 177s - loss: 1.9916 - acc: 0.3037 - val_loss: 1.8654 - val_acc: 0.3828\n",
      "Epoch 3/20\n",
      "11314/11314 [==============================] - 181s - loss: 1.5137 - acc: 0.4449 - val_loss: 1.4921 - val_acc: 0.5133\n",
      "Epoch 4/20\n",
      "11314/11314 [==============================] - 182s - loss: 1.1167 - acc: 0.6070 - val_loss: 1.3270 - val_acc: 0.5593\n",
      "Epoch 5/20\n",
      "11314/11314 [==============================] - 180s - loss: 0.8647 - acc: 0.7036 - val_loss: 1.1327 - val_acc: 0.6518\n",
      "Epoch 6/20\n",
      "11314/11314 [==============================] - 180s - loss: 0.6720 - acc: 0.7757 - val_loss: 1.0200 - val_acc: 0.6887\n",
      "Epoch 7/20\n",
      "11314/11314 [==============================] - 181s - loss: 0.5374 - acc: 0.8269 - val_loss: 0.8951 - val_acc: 0.7260\n",
      "Epoch 8/20\n",
      "11314/11314 [==============================] - 181s - loss: 0.4317 - acc: 0.8632 - val_loss: 0.8843 - val_acc: 0.7339\n",
      "Epoch 9/20\n",
      "11314/11314 [==============================] - 185s - loss: 0.3440 - acc: 0.8906 - val_loss: 0.8010 - val_acc: 0.7600\n",
      "Epoch 10/20\n",
      "11314/11314 [==============================] - 184s - loss: 0.2688 - acc: 0.9156 - val_loss: 0.7563 - val_acc: 0.7666\n",
      "Epoch 11/20\n",
      "11314/11314 [==============================] - 178s - loss: 0.2184 - acc: 0.9334 - val_loss: 0.7849 - val_acc: 0.7689\n",
      "Epoch 12/20\n",
      "11314/11314 [==============================] - 180s - loss: 0.1740 - acc: 0.9473 - val_loss: 0.8139 - val_acc: 0.7604\n",
      "Epoch 13/20\n",
      "11314/11314 [==============================] - 180s - loss: 0.1367 - acc: 0.9570 - val_loss: 0.9951 - val_acc: 0.7179\n",
      "Epoch 14/20\n",
      "11314/11314 [==============================] - 182s - loss: 0.1044 - acc: 0.9683 - val_loss: 0.7785 - val_acc: 0.7900\n",
      "Epoch 15/20\n",
      "11314/11314 [==============================] - 180s - loss: 0.0877 - acc: 0.9725 - val_loss: 0.8641 - val_acc: 0.7828\n",
      "Epoch 16/20\n",
      "11314/11314 [==============================] - 184s - loss: 0.0769 - acc: 0.9775 - val_loss: 0.9341 - val_acc: 0.7637\n",
      "Epoch 17/20\n",
      "11314/11314 [==============================] - 188s - loss: 0.0551 - acc: 0.9847 - val_loss: 0.8329 - val_acc: 0.7963\n",
      "Epoch 18/20\n",
      "11314/11314 [==============================] - 183s - loss: 0.0521 - acc: 0.9860 - val_loss: 0.8523 - val_acc: 0.7997\n",
      "Epoch 19/20\n",
      "11314/11314 [==============================] - 178s - loss: 0.0590 - acc: 0.9861 - val_loss: 1.0596 - val_acc: 0.7645\n",
      "Epoch 20/20\n",
      "11314/11314 [==============================] - 183s - loss: 0.0479 - acc: 0.9883 - val_loss: 0.9919 - val_acc: 0.7821\n",
      "CPU times: user 1h 32min 3s, sys: 6min 59s, total: 1h 39min 2s\n",
      "Wall time: 1h 43s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f748b70>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=20,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geat! So we've built a neural network that learns to classify documents.\n",
    "Yet, as we can see, it takes *ages* to train (and even with a GPU: a lot more time) compared to all former models. Worse, it does not achieve the accuracy of the best models from day 1, either. The above model can be made to achieve around 80% max. accuracy if you can use 300- or 100-dimensional word-embeddings and train the model long enough (15-20 epochs). With 50d embeddings and no embedding layer training, you need to run for about 20-30 epochs to converge on around 67% accuracy. So these results are a long shot from the 90% that is possible on this data and the 85% we got from the very ad-hoc \"blitz-classification-experiment\" on day one. And, all that despite providing the model with the unsupervised, neural word embeddings learned from the whole Wikipedia.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Overall, this final notebook is mostly here to just show you that you should probably focus on simple things first:\n",
    "\n",
    "- Learn your own embeddings (ideally from a text collection matching your target domain), and make sure to add those collocations we discussed\n",
    "- Replace the old-school TF-IDF vectors with modern-day neural embeddings, but then learn an \"old school\" ML model, at least at first\n",
    "\n",
    "You will save yourself tons of time and pain, and still can tell your group leader/CTO/CEO that you built them a neural model\n",
    "(Remember, the word embeddings are also called \"neural embeddings\" and were learned from a (admittedly rather shallow...) perceptron.)\n",
    "\n",
    "And so, the challenge remains: How to actually beat the state-of-the-art in text classification with deep learning?\n",
    "At the very least: \"Its tricky\"!\n",
    "Very [well-designed conv nets](https://link.springer.com/chapter/10.1007/978-3-662-44851-9_28), and rather [recent research on belief networks](https://link.springer.com/article/10.1007/s00521-016-2401-x) only very recently managed to claim to achieve the same ballpark results as the state-of-the-art results on \"old school\" models. But most of even the [current deep learning research](https://arxiv.org/pdf/1601.02733.pdf) still does not beat those \"old\" models on these two datasets.\n",
    "(Which is not to say that stuff like VAEs are very cool though - and probalby would unfold their \"full beauty\" if you had a much larger dataset - see next.)\n",
    "And probably by using LSTMs or GRU-RNNs to train sequence models might get you even beyond the state-of-the-art - if you have the resources and the data to even think of that, and an extensive amount of time to develop your classifier.\n",
    "(And then have the resources to run inference on that mega-model in production, too...)\n",
    "\n",
    "## Take-home message\n",
    "\n",
    "In the opinion of your instructor, particularly the deep learning literature sadly is *littered* with evaluation results that claim to beat all former state-of-the-art, but indeed are quite frequently not much better (or ex-aequo, and often even worse) -- With computer vision, machine translation, and dependency parsing being the famous cases where deep learning indeed has \"pushed the envelope\" by a substantial margin *on the **same**, **public** (and often, small) community datasets* for evaluating the approach and comparing it to existing methods. And nearly no paper at all discusses how much more resources go into setting up, developing, training, and using deep learning models.\n",
    "\n",
    "That being said, many other applications (apart from CV, MT, and DP) can profit from deep learning for the following reason:\n",
    "*Iff* you have much more training data (thousands, or even millions of examples per label), then, because deep learning can easily be scaled to work on such gigantic datasets, it indeed beats other methods (Support Vector Machines, Random Forrests, Nearest Neighbours, Gradient Boosting, etc.).\n",
    "\n",
    "However, your instructor's personal experience is that unless you work for any (well known...) company that can spend hundreds of thousands (or much more!) of Dollars on developing such \"giga-sets\", you will not get to work with annotated data in that size-range.\n",
    "So, at the end of the day - deep learning is *very cool*, but you should take much of it with a *very large* grain of salt regarding how much time and money you can invest (particularly, given the current hype about it)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
